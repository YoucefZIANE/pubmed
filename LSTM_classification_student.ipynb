{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_classification_student.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aneuraz/intro-keras/blob/master/LSTM_classification_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8JC4LYa8jjE",
        "colab_type": "text"
      },
      "source": [
        "# Classification de texte en deep learning (LSTM et convolution) \n",
        "\n",
        "## But de la tâche \n",
        "\n",
        "A partir d'un dataset d'articles PUBMED, le but est de classifier les articles dans des catégories thématiques en fonction de leur titre. \n",
        "\n",
        "Après une phase de préprocessing du texte, nous entrainerons un modèle à base de convolutions, puis un modèle à base de réseau de neurones récurrents (LSTM) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAervZV0djbe",
        "colab_type": "text"
      },
      "source": [
        "## Cloner le repo https://github.com/aneuraz/intro-keras.git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjN4D9A9EGB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/aneuraz/intro-keras.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFIznPvCc6o_",
        "colab_type": "text"
      },
      "source": [
        "## Import des libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TrZgMq4fReH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import json \n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq5LGnyzduPx",
        "colab_type": "text"
      },
      "source": [
        "## Chargement des données\n",
        "\n",
        "Toutes les données chargées se situent dans le répertoire `/content/`.\n",
        "Les données sont dans un fichier JSON."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYQ5UeLqfnM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/intro-keras/ai_pub_samp.json','r') as f:\n",
        "  data = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_PoXhiLfyAD",
        "colab_type": "code",
        "outputId": "ad935681-026b-4325-f6e2-72febe93e3dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "source": [
        "data[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Cat_2013': 'C',\n",
              " 'Cat_2014': 'C',\n",
              " 'Cat_2015': 'C',\n",
              " 'Cat_2016': 'C',\n",
              " 'Cat_2017': 'B',\n",
              " 'Disciplines': ['XQ'],\n",
              " 'ESSN': '1873-3557',\n",
              " 'IF_2013': '2.129',\n",
              " 'IF_2014': '2.353',\n",
              " 'IF_2015': '2.653',\n",
              " 'IF_2016': '2.536',\n",
              " 'IF_2017': '2.88',\n",
              " 'ISSN': '1386-1425',\n",
              " 'ISSN_online': '1873-3557',\n",
              " 'ISSN_print': '1386-1425',\n",
              " 'IsoAbbr': 'Spectrochim Acta A Mol Biomol Spectrosc',\n",
              " 'JrId': 20555,\n",
              " 'MedAbbr': 'Spectrochim Acta A Mol Biomol Spectrosc',\n",
              " 'NLMid': '9602533',\n",
              " 'Titre': 'Spectrochim Acta A Mol Biomol Spectrosc',\n",
              " 'abstract': 'In this research, ZnO nanoparticle loaded on activated carbon (ZnO-NPs-AC) was synthesized simply by a low cost and nontoxic procedure. The characterization and identification have been completed by different techniques such as SEM and XRD analysis. A three layer artificial neural network (ANN) model is applicable for accurate prediction of dye removal percentage from aqueous solution by ZnO-NRs-AC following conduction of 270 experimental data. The network was trained using the obtained experimental data at optimum pH with different ZnO-NRs-AC amount (0.005-0.015 g) and 5-40 mg/L of sunset yellow dye over contact time of 0.5-30 min. The ANN model was applied for prediction of the removal percentage of present systems with Levenberg-Marquardt algorithm (LMA), a linear transfer function (purelin) at output layer and a tangent sigmoid transfer function (tansig) in the hidden layer with 6 neurons. The minimum mean squared error (MSE) of 0.0008 and coefficient of determination (R(2)) of 0.998 were found for prediction and modeling of SY removal. The influence of parameters including adsorbent amount, initial dye concentration, pH and contact time on sunset yellow (SY) removal percentage were investigated and optimal experimental conditions were ascertained. Optimal conditions were set as follows: pH, 2.0; 10 min contact time; an adsorbent dose of 0.015 g. Equilibrium data fitted truly with the Langmuir model with maximum adsorption capacity of 142.85 mg/g for 0.005 g adsorbent. The adsorption of sunset yellow followed the pseudo-second-order rate equation.',\n",
              " 'authors': ['Maghsoudi, M',\n",
              "  'Ghaedi, M',\n",
              "  'Zinali, A',\n",
              "  'Ghaedi, A M',\n",
              "  'Habibi, M H'],\n",
              " 'categories': ['SPECTROSCOPY'],\n",
              " 'journal': 'Spectrochimica acta. Part A, Molecular and biomolecular spectroscopy',\n",
              " 'keywords': ['Adsorption',\n",
              "  'Algorithms',\n",
              "  'Azo Compounds',\n",
              "  'Charcoal',\n",
              "  'Coloring Agents',\n",
              "  'Hydrogen-Ion Concentration',\n",
              "  'Kinetics',\n",
              "  'Microscopy, Electron, Scanning',\n",
              "  'Nanotubes',\n",
              "  'Neural Networks (Computer)',\n",
              "  'Time Factors',\n",
              "  'Water Pollutants, Chemical',\n",
              "  'X-Ray Diffraction',\n",
              "  'Zinc Oxide'],\n",
              " 'pmid': '24995412',\n",
              " 'title': 'Artificial neural network (ANN) method for modeling of sunset yellow dye adsorption using zinc oxide nanorods loaded on activated carbon: Kinetic and isotherm study.',\n",
              " 'year': '2015'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJrtCMBYeFvS",
        "colab_type": "text"
      },
      "source": [
        "## TODO: Extraire les titres et les catégories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MOQZkZxfzIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mettre le titre en minuscule dans la variable X\n",
        "\n",
        "\n",
        "# mettre la catégorie (1e élément de la liste) dans la variable Y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFN2trGeefrh",
        "colab_type": "text"
      },
      "source": [
        "## TODO: Calculer la longueur maximale des titres dans le dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voXtnTBEgIXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# longueur maximale des titres, variable max_len\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv9MBmJsexz9",
        "colab_type": "text"
      },
      "source": [
        "## TODO: Diviser le dataset en train (X_train, Y_train) et test (X_test, Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIcFAOvDgz0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train, Y_train\n",
        "\n",
        "\n",
        "# X_test, Y_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3DcNJBxggZW",
        "colab_type": "text"
      },
      "source": [
        "## Transformer la variable Y en vecteur numerique\n",
        "\n",
        "[\"Cat 1\", \"Cat 2\"] -> [0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ0dnsZuie3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creer un mapping cat_2_id\n",
        "\n",
        "# creer un reverse mapping id_2_cat\n",
        "\n",
        "# calculer la taille du vocabulaire cat_vocab\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHLYjSWyigQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocesser les X_train et X_test en X_train_id et X_test_id\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I-bA1Dihtik",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizer les titres\n",
        "\n",
        "Pour cela vous pouvez utiliser la fonction `Tokenizer` de keras\n",
        "\n",
        "Le but est de transformer les textes en un vecteur numérique\n",
        "\n",
        "texte -> liste de tokens -> vecteur numérique\n",
        "\n",
        "\"Miaou le chat\" -> [\"Miaou\", \"le\", chat\"] -> [1, 2, 3]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-8LPBeJjJl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Créer le tokenizer\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07eGO99-j5ZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Entrainer le tokenizer sur le train set \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nI6qhqplcKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transformer les textes en vecteurs numeriques à l'aide du tokenizer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao-Lj9nljVAH",
        "colab_type": "text"
      },
      "source": [
        "## Faire un padding des sequences obtenues pour qu'elles aient toutes la même taille (cf la fonction `pad_sequences`)\n",
        "\n",
        "[1, 2, 3]       -> [0, 0, 1, 2 ,3]\n",
        "\n",
        "[4, 5, 6, 7, 8] -> [4, 5, 6, 7, 8]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aMIpV1clj8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Padding des sequences \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZHz3Dljlk19",
        "colab_type": "code",
        "outputId": "721b18ca-8578-4fa3-950b-5866639d0169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "X_train_seq[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,   21,    9,   16,  945,   48,    4,\n",
              "          79,    1, 5677, 1973, 2247, 2614,    7, 2248, 2615, 5678, 3995,\n",
              "          13, 1586,  946, 1974,    2, 5679,   37],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,  462,    1,   74,  124,   48,    3,\n",
              "         615,  616, 3135,   10,    5,  693,   48],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,  135,   14,    1,  589,\n",
              "         143,  327,    3, 3136, 1337,   64, 2616],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,  802, 1587,    1, 5680, 1449,\n",
              "        3996,    2, 3137,   91,  220,   45,   65],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  565,\n",
              "           1, 3138,  656,    4,  328,  765,    7,  307,  249,  484,   14,\n",
              "           3, 2617,  657,    2,  520,   26,   42]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMrlhsz_l-Pz",
        "colab_type": "code",
        "outputId": "dec19961-a92c-43fc-bf10-978ee9d05fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "X_test_seq[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,   23,   18, 3459,\n",
              "        9708,  197,  117,    3,    6,   85, 1354],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,   72,   89,    3,  911,  818,    3,    5, 1288,\n",
              "           1,  116,   14,    1,  255, 9110,  412],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,   46,   18,   23,\n",
              "         262,  142, 1309,    3,   19,  823,   33],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,  368,    2,  561,  384,  701,    1, 3827,  544,   11,   13,\n",
              "          45,   65, 1072,    3,   40,  696, 1313],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "        1548,   99,   22,    5,  997,  179,  372]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkstuTOO05sw",
        "colab_type": "code",
        "outputId": "38d6e1cb-12b1-41e3-d39b-c71d2605530d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "Y_test_id[0:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([30, 30, 11,  6,  6,  6,  7, 22, 22, 45, 22, 11, 21, 44,  7, 15, 56,\n",
              "        3,  8,  3, 13, 70, 11, 56, 11, 30, 11, 44, 31, 15, 79, 11,  6, 11,\n",
              "        4, 13,  6, 65, 49, 56, 12,  6,  6,  6,  6, 22, 86, 26, 11,  3, 12,\n",
              "       19, 11,  6, 21, 56,  7,  6, 11,  2, 46,  7,  3, 21, 13,  9,  3,  6,\n",
              "       19, 49,  2, 11, 27, 13, 71, 59, 13,  6, 13, 15,  3,  6,  6, 11, 22,\n",
              "        6, 13, 28,  8, 22,  6, 11,  6, 21,  6, 50, 46, 14, 17, 21])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KCGGWdoj_I2",
        "colab_type": "text"
      },
      "source": [
        "# Réseau de convolution pour la classification de texte\n",
        "\n",
        "Les réseaux convolutionnels peuvent également être utiliser pour le texte et notamment pour la classification de texte. Ici nous allons construire un CNN sur le même modèle que pour les images avec quelques petites spécificités. \n",
        "\n",
        "Comme le texte est une séquence de mots, il s'agit d'une séquence en 1 dimension. Nous appliquerons donc une convolution en 1D. \n",
        "\n",
        "Pour traiter du texte, la première couche de notre réseau va être constituée par une couche d'embedding. \n",
        "\n",
        "Pour rappel, le word embedding consiste à projeter les tokens dans un espace vectoriel qui va minimiser la distance entre les tokens qui sont utilisés dans des contextes similaires (et qui ont un sens proche ? )\n",
        "\n",
        "![Texte alternatif…](https://www.ibm.com/blogs/research/wp-content/uploads/2018/10/WMEFig1.png)\n",
        "\n",
        "Les embeddings peuvent être calculés de diverses façons. Par exemple word2vec, un des plus célèbres, se base sur 2 algorithmes frères Skip-gram et CBOW\n",
        "\n",
        "![Texte alternatif…](https://pathmind.com/images/wiki/word2vec_diagrams.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ-GICwamQGd",
        "colab_type": "text"
      },
      "source": [
        "Pour information, il existe aujourd'hui des algorithmes plus performants que word2vec comme [Fasttext](https://fasttext.cc) qui prend en compte des informations de sous-mots ou la famille des embeddings contextuels comme [ELMo](https://allennlp.org/elmo) ou [BERT](https://arxiv.org/abs/1810.04805) qui prennent en compte le contexte d'utilisation du mot pour calculer son vecteur. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L92Fw-TnUG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Créer le modèle avec au minimum\n",
        "# Embedding \n",
        "# Dropout\n",
        "# Convolution\n",
        "# Maxpooling\n",
        "# Dense\n",
        "# Activation\n",
        "# Classifieur (Dense + activation softmax)\n",
        "\n",
        "\n",
        "# Compiler le modèle \n",
        "\n",
        "# Afficher le summary du modèle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8TD5CkVnt-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitter le modèle \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlAKERnqy-Wu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluer le modèle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWqvNb5Jp1Uo",
        "colab_type": "text"
      },
      "source": [
        "# LSTM pour la classification de texte\n",
        "\n",
        "Il est également possible d'utiliser un autre type de réseau de neurones pour effectuer ce genre de tâches: les réseaux de neurones récurrents ou RNN.\n",
        "\n",
        "Les RNN sont conçus pour gérer les séquences. Le réseau prend les tokens un par un et calcule une représentation de la séquence à chaque pas qui tiens compte de tous les pas précédents \n",
        "\n",
        "![Texte alternatif…](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/450px-Recurrent_neural_network_unfold.svg.png)\n",
        "\n",
        "\n",
        "Il existe différents types de RNN. Ici nous utiliserons les Long Short-Term Memory (LSTM) qui permettent d'améliorer les performances sur des séquences longues avec une série de \"gates\". \n",
        "\n",
        "![Texte alternatif…](http://dprogrammer.org/wp-content/uploads/2019/04/RNN-vs-LSTM-vs-GRU-1200x361.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6mO7VTjocwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Créer un réseau à base de LSTM avec au minimum:\n",
        "# Embedding\n",
        "# Dropout\n",
        "# LSTM\n",
        "# Dropout\n",
        "# Classifieur\n",
        "\n",
        "\n",
        "# Compiler le modèle \n",
        "\n",
        "# Afficher le summary du modèle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VphlgAhjqSCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitter le modèle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVEY_kx3qSnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluer le modèle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x28y_mZsnbx",
        "colab_type": "text"
      },
      "source": [
        "# Utiliser les embeddings pré-entrainés\n",
        "\n",
        "Pour améliorer la qualité de la représentation des mots, il est possible d'entrainer les embeddings sur de larges corpus de textes non annotés (typiquement Wikipedia). Ces modèles sont souvent disponibles en ligne et il est possible de les télécharger. Ici nous allons utiliser des embeddings [Glove](https://nlp.stanford.edu/projects/glove/) de taille 50d (pour des raisons techniques mais il vaut mieux utiliser des dimensions plus importantes entre 100 et 300) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXZjI4xX1xn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fonction permettant de charger un embedding \n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def load_glove_embeddings(fp, embedding_dim, include_empty_char=True):\n",
        "    \"\"\"\n",
        "    Loads pre-trained word embeddings (GloVe embeddings)\n",
        "        Inputs: - fp: filepath of pre-trained glove embeddings\n",
        "                - embedding_dim: dimension of each vector embedding\n",
        "                - generate_matrix: whether to generate an embedding matrix\n",
        "        Outputs:\n",
        "                - word2coefs: Dictionary. Word to its corresponding coefficients\n",
        "                - word2index: Dictionary. Word to word-index\n",
        "                - embedding_matrix: Embedding matrix for Keras Embedding layer\n",
        "    \"\"\"\n",
        "    # First, build the \"word2coefs\" and \"word2index\"\n",
        "    word2coefs = {} # word to its corresponding coefficients\n",
        "    word2index = {} # word to word-index\n",
        "    with open(fp) as f:\n",
        "        for idx, line in enumerate(f):\n",
        "            try:\n",
        "                data = [x.strip().lower() for x in line.split()]\n",
        "                word = data[0]\n",
        "                coefs = np.asarray(data[1:embedding_dim+1], dtype='float32')\n",
        "                word2coefs[word] = coefs\n",
        "                if word not in word2index:\n",
        "                    word2index[word] = len(word2index)\n",
        "            except Exception as e:\n",
        "                print('Exception occurred in `load_glove_embeddings`:', e)\n",
        "                continue\n",
        "        # End of for loop.\n",
        "    # End of with open\n",
        "    if include_empty_char:\n",
        "        word2index[''] = len(word2index)\n",
        "    # Second, build the \"embedding_matrix\"\n",
        "    # Words not found in embedding index will be all-zeros. Hence, the \"+1\".\n",
        "    vocab_size = len(word2coefs)+1 if include_empty_char else len(word2coefs)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, idx in word2index.items():\n",
        "        embedding_vec = word2coefs.get(word)\n",
        "        if embedding_vec is not None and embedding_vec.shape[0]==embedding_dim:\n",
        "            embedding_matrix[idx] = np.asarray(embedding_vec)\n",
        "    # return word2coefs, word2index, embedding_matrix\n",
        "    return word2index, np.asarray(embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiWQhLQ81yaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Télécharger les embeddings\n",
        "\n",
        "!wget https://github.com/kmr0877/IMDB-Sentiment-Classification-CBOW-Model/raw/master/glove.6B.50d.txt.gz\n",
        "!gunzip /content/glove.6B.50d.txt.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0CPaa_r3B3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Charger les embeddings à l'aide de la fonction load_glove_embeddings\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1T2Kdot3GKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ecrire une fonction de tokenization custom pour preprocesser les textes\n",
        "\n",
        "\n",
        "# Encoder les textes avec la fonction custom\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF40dcAH4HEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Padding des sequences\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqxsq-rY527k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Créer un modèle en chargeant les poids des embeddings dans le layer Embedding\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VTS-j-v6RLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitter le modèle\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikDrLZyC6b1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluer le modèle\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}